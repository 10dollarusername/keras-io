{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Supervised Contrastive Learning\n",
    "\n",
    "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
    "**Date created:** 2020/11/01<br>\n",
    "**Last modified:** 2020/11/01<br>\n",
    "**Description:** Using supervised contrastive learning for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
    "(Prannay Khosla et al.) is a training methodology that outperforms cross-entropy\n",
    "on supervised learning tasks.\n",
    "\n",
    "Essentially, training an image classification model with Supervised Contrastive Learning\n",
    "is peformed in two phases:\n",
    "\n",
    "  1. Pre-training an encoder to generate feature vectors for input images such that feature\n",
    "    vectors of images in the same class will be more similar compared feature vectors of\n",
    "    images in other classes.\n",
    "  2. Training a classifier on top of the freezed encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_train, y_test = tf.squeeze(y_train), tf.squeeze(y_test)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build the encoder model\n",
    "\n",
    "The encoder model takes the image as an input and produce a 128-dimension feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_encoder():\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=input_shape),\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "TEMPERATURE = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build the classification model\n",
    "\n",
    "The classification model adds a fully-connected layer on top of the encoder, plus a\n",
    "softmax layer with the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = tf.keras.layers.Dropout(DROPOUT)(features)\n",
    "    features = tf.keras.layers.Dense(64)(features)\n",
    "    features = tf.keras.layers.Dropout(DROPOUT)(features)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Experiment 1: Train the baseline classification model\n",
    "\n",
    "In this experiment, a baseline classifier is trained normally, i.e., the encoder and the\n",
    "classifier parts are trained together as a single model to minimize cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "encoder = create_encoder()\n",
    "classifier = create_classifier(encoder)\n",
    "classifier.summary()\n",
    "\n",
    "history = classifier.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We get to ~70.1% validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Experiment 2: Use supervised contrastive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 1. Supervised contrastive learning loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_supervised_contrastive_loss_fn(temperature=1):\n",
    "    def supervised_contrastive_loss(labels, feature_vectors):\n",
    "\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            temperature,\n",
    "        )\n",
    "\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "    return supervised_contrastive_loss\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 2. Pretrain the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "encoder = create_encoder()\n",
    "encoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=make_supervised_contrastive_loss_fn(temperature=TEMPERATURE),\n",
    ")\n",
    "\n",
    "history = encoder.fit(\n",
    "    x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 3. Train the classifier with the freezed encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "classifier = create_classifier(encoder, trainable=False)\n",
    "history = classifier.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We get to ~72.6% validation accuracy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "supervised-contrastive-learning",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}